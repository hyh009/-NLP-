{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "cupoy_env",
      "language": "python",
      "name": "cupoy_env"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "以Pytorch進行自由數據讀取_作業.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMRg4ZYzNots"
      },
      "source": [
        "### 作業目的: 熟練自定義collate_fn與sampler進行資料讀取\n",
        "\n",
        "本此作業主要會使用[IMDB](http://ai.stanford.edu/~amaas/data/sentiment/)資料集利用Pytorch的Dataset與DataLoader進行\n",
        "客製化資料讀取。\n",
        "下載後的資料有分成train與test，因為這份作業目的在讀取資料，所以我們取用train部分來進行練習。\n",
        "(請同學先行至IMDB下載資料)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2O2tGG8TNotv"
      },
      "source": [
        "### 載入套件"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xJ9rFoUONotw",
        "outputId": "72b70ee3-d372-4ed5-922a-a89db4bbe9b2"
      },
      "source": [
        "# Import torch and other required modules\n",
        "import glob\n",
        "import torch\n",
        "import re\n",
        "import nltk\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader, RandomSampler\n",
        "from sklearn.datasets import load_svmlight_file\n",
        "from nltk.corpus import stopwords\n",
        "import os\n",
        "\n",
        "nltk.download('stopwords') #下載stopwords\n",
        "nltk.download('punkt') #下載word_tokenize需要的corpus"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EN7IBaquOwzT"
      },
      "source": [
        "!wget https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xvf /content/aclImdb_v1.tar.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Km6dbt1BNotx"
      },
      "source": [
        "### 探索資料與資料前處理\n",
        "這份作業我們使用test資料中的pos與neg\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b4IlnBC_PAtJ",
        "outputId": "692e508a-243e-4f7c-927c-353bd3872f2f"
      },
      "source": [
        "# 讀取字典，這份字典為review內所有出現的字詞\n",
        "###<your code>###\n",
        "vocab = []\n",
        "vocab_dic = {}\n",
        "id=0\n",
        "with open('./aclImdb/imdb.vocab','r',encoding='utf-8') as f:\n",
        "  for word in f.readlines():\n",
        "    vocab.append(word.strip())\n",
        "vocab = list(set(vocab))\n",
        "print(f\"vocab length before removing stopwords: {len(vocab)}\")\n",
        "# 以nltk stopwords移除贅字，過多的贅字無法提供有用的訊息，也可能影響模型的訓練\n",
        "###<your code>###\n",
        "stop_words = set(stopwords.words('english'))\n",
        "vocab = [word for word in vocab if word not in stop_words]\n",
        "print(f\"vocab length after removing stopwords: {len(vocab)}\")\n",
        "\n",
        "# 將字典轉換成dictionary\n",
        "### <your code>###\n",
        "for word in vocab:\n",
        "  if word not in vocab_dic:\n",
        "    vocab_dic[word]=id\n",
        "    id+=1"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "vocab length before removing stopwords: 89527\n",
            "vocab length after removing stopwords: 89356\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KiFe9gSMNoty",
        "outputId": "70437f2a-fa75-4ec4-80d0-4dea2dfe1f1f"
      },
      "source": [
        "# 將資料打包成(x, y)配對，其中x為review的檔案路徑，y為正評(1)或負評(0)\n",
        "# 這裡將x以檔案路徑代表的原因是讓同學練習不一次將資料全讀取進來，若電腦記憶體夠大(所有資料檔案沒有很大)\n",
        "# 可以將資料全一次讀取，可以減少在訓練時I/O時間，增加訓練速度\n",
        "\n",
        "###<your code>###\n",
        "review_pairs = []\n",
        "folder_list = {'pos':1,'neg':0}\n",
        "path = './aclImdb/test'\n",
        "for folder in folder_list:\n",
        "  glob_pattern = os.path.join(path, folder, '*.txt')\n",
        "  for file_path in glob.glob(glob_pattern):\n",
        "    review_pairs.append((file_path,folder_list[folder]))\n",
        "\n",
        "print(review_pairs[:2])\n",
        "print(f\"Total reviews: {len(review_pairs)}\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('./aclImdb/test/pos/8274_10.txt', 1), ('./aclImdb/test/pos/10571_10.txt', 1)]\n",
            "Total reviews: 25000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-eJJfSOXNoty"
      },
      "source": [
        "### 建立Dataset, DataLoader, Sampler與Collate_fn讀取資料\n",
        "這裡我們會需要兩個helper functions，其中一個是讀取資料與清洗資料的函式(load_review)，另外一個是生成詞向量函式\n",
        "(generate_vec)，注意這裡我們用來產生詞向量的方法是單純將文字tokenize(為了使產生的文本長度不同，而不使用BoW)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKGTPWCTNoty"
      },
      "source": [
        "def load_review(review_path):\n",
        "    \n",
        "  ###<your code>###\n",
        "  with open(review_path,'r',encoding='utf-8')as f:\n",
        "    review = f.read()\n",
        "    review = nltk.word_tokenize(review)\n",
        "    review = [re.sub('[^A-Za-z\\-]+','',word) for word in review if word not in stop_words and len(word)>2]\n",
        "  return review\n",
        "\n",
        "def generate_vec(review, vocab_dic):\n",
        "  ### <your code> ###\n",
        "  review_vector = [vocab_dic[word] for word in review if word in vocab_dic]\n",
        "  return review_vector"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ln9Pa42wNotz"
      },
      "source": [
        "#建立客製化dataset\n",
        "\n",
        "class dataset(Dataset):\n",
        "  '''custom dataset to load reviews and labels\n",
        "  Parameters\n",
        "  ----------\n",
        "  data_pairs: list\n",
        "      tuple of all review-label pairs\n",
        "  vocab: dict\n",
        "      list of vocabularies and word_id\n",
        "  '''\n",
        "  ### <your code> ###\n",
        "  def __init__(self,data_pairs,vocab):\n",
        "    self.data_pairs = data_pairs\n",
        "    self.vocab = vocab\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.data_pairs)\n",
        "\n",
        "  def __getitem__(self,idx):\n",
        "    path,label = self.data_pairs[idx]\n",
        "    review = load_review(path)\n",
        "    review_vector = generate_vec(review, self.vocab)\n",
        "    return torch.tensor(review_vector), torch.tensor(label)\n",
        "    \n",
        "#建立客製化collate_fn，將長度不一的文本pad 0 變成相同長度\n",
        "def collate_fn(batch):\n",
        "  ### <your code> ###\n",
        "  corpus, labels = zip(*batch) \n",
        "    \n",
        "  ### create pads for corpus ###\n",
        "  lengths = [len(review) for review in corpus]\n",
        "  max_length = max(lengths)\n",
        "\n",
        "  batch_corpus = []\n",
        "  \n",
        "  for i in range(len(corpus)):\n",
        "      # pad corpus\n",
        "      tmp_pads = torch.zeros(max_length)\n",
        "      tmp_pads[:lengths[i]] = corpus[i]\n",
        "      batch_corpus.append(tmp_pads.view(1,-1))\n",
        "\n",
        "  return torch.cat(batch_corpus,dim=0), torch.tensor(labels) , torch.tensor(lengths)\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CzZQdBj7Notz",
        "outputId": "4d17f9ee-d399-4802-82de-029551cd10e4"
      },
      "source": [
        "# 使用Pytorch的RandomSampler來進行indice讀取並建立dataloader\n",
        "### <your code> ###\n",
        "custom_dataset = dataset(review_pairs,vocab_dic)\n",
        "custom_dataloader = DataLoader(custom_dataset, \n",
        "                batch_size=4, \n",
        "                sampler=RandomSampler(custom_dataset), \n",
        "                collate_fn=collate_fn)\n",
        "next(iter(custom_dataloader))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[77078., 38213., 19969., 38835., 30956.,   813.,     0.,     0.,     0.,\n",
              "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
              "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
              "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
              "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
              "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
              "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
              "              0.],\n",
              "         [38213., 25526., 42197., 40206., 16532., 89194., 61853., 81324., 38213.,\n",
              "           1097., 38213., 59376., 83610.,   407., 55985., 57415., 38611., 61853.,\n",
              "          72759.,  6919., 81324., 60020., 67104., 85964., 43822., 87576., 74176.,\n",
              "          54098., 19969., 85396.,  6635., 43822., 47502., 79574., 38199., 40975.,\n",
              "          24020., 79574., 16822., 21779., 38213., 43822., 74967.,  2790.,  5084.,\n",
              "          22074., 35848.,  9294., 34567., 78784., 74967., 23752., 45965., 26175.,\n",
              "          51825., 82278., 17126., 82775., 74176., 38213., 75519., 61853., 57472.,\n",
              "          76679.],\n",
              "         [43938., 64929.,  4859., 71682., 58066., 78470., 29383., 75384., 19969.,\n",
              "           4186., 49440., 78787., 70677.,  8577., 27517., 43084., 44323., 84663.,\n",
              "          22238.,  9585., 63558., 59500.,  2990., 11966., 63094.,  4186., 57415.,\n",
              "          57282., 34475., 17523., 46042., 30665., 38181., 77571., 34625., 63413.,\n",
              "          52173., 58876., 25542., 61853., 51228., 63558., 25542.,  9294., 87724.,\n",
              "           8744., 40872., 72565., 26914., 61541., 51228., 78422., 71682., 53579.,\n",
              "          63909., 23305., 82775., 29559.,     0.,     0.,     0.,     0.,     0.,\n",
              "              0.],\n",
              "         [19619., 25542., 67104., 67954., 43261., 17523., 28590., 79902., 88890.,\n",
              "          57826., 32163., 24480., 15921., 36085., 24758., 57196., 32427., 68468.,\n",
              "          32163., 45167., 72503.,  9225.,  5084., 39280., 63710., 86922., 86449.,\n",
              "          20074.,  2179., 66275., 45617., 77078., 79174., 51224., 31469., 38181.,\n",
              "          77571., 80363., 81746., 80251., 76577., 73022.,  8744., 23305., 25167.,\n",
              "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
              "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
              "              0.]]), tensor([1, 0, 0, 1]), tensor([ 6, 64, 58, 45]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    }
  ]
}